# OnPrem RAG Featuring LangServe, vLLM Endpoints, and Ollama Embedding Model

This LangServe application will need: 

- [vLLM Endpoint](https://docs.vllm.ai/en/v0.5.5/serving/deploying_with_docker.html)
- [Ollama Model Served](https://ollama.com/blog/embedding-models)
- [Qdrant](https://qdrant.tech/documentation/quickstart/)
