{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "\n",
    "%pip install -qU langchain-community langchain-core==0.2.40 langchain_experimental langchain-google-community langchain-google-genai langchain-huggingface==0.0.3 langchain-openai langchain-qdrant langchain-google-vertexai\n",
    "%pip install -qU -q chainlit==1.1.302\n",
    "%pip install -qU -q docx2txt\n",
    "%pip install -qU google-cloud-aiplatform\n",
    "%pip install -qU google-cloud-discoveryengine\n",
    "%pip install -qU nltk\n",
    "%pip install -qU openpyxl\n",
    "%pip install -qU pymupdf\n",
    "%pip install -qU python-dotenv\n",
    "%pip install -qU ragas==0.1.20\n",
    "%pip install -qU tqdm\n",
    "\n",
    "# Verify installed packages have compatible dependencies\n",
    "%pip check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download punkt_tab module that is used for sentence tokenizaiton\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get environment variables\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Bucket: 395640738565_us_import_content_with_faq_csv>, <Bucket: aie4-demo-docs>]\n"
     ]
    }
   ],
   "source": [
    "# Set Google user permissions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If running in Colab, use the permissions of the currently authenticated user\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()\n",
    "\n",
    "# If not, set the GOOGLE_APPLICATION_CREDENTIALS to the service account credentials file \n",
    "else:\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = 'credentials.json'\n",
    "\n",
    "#####\n",
    "\n",
    "def test_google_perms():\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # Now, you can use the Google Cloud client libraries\n",
    "    client = storage.Client()\n",
    "\n",
    "    # List all buckets in your project\n",
    "    buckets = list(client.list_buckets())\n",
    "    print(buckets)\n",
    "\n",
    "test_google_perms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=os.environ['PROJECT_ID'], location=os.environ['REGION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"## Understanding the Magic Behind AI: A Simple Explanation\\n\\nAI, or Artificial Intelligence, might sound like something out of a sci-fi movie, but it's actually based on some pretty straightforward principles. Here's a simplified breakdown:\\n\\n**1. Learning from Data:**\\n\\n* **Think of a baby learning to walk:** They observe, try, fall, and eventually learn the right movements.\\n* **AI does the same, but with data:** It's fed vast amounts of information (like images, text, or code) and analyzes it to identify patterns and relationships.\\n* **The more data it receives, the better it understands and learns.**\\n\\n**2. Algorithms: The Recipe for Intelligence:**\\n\\n* **Algorithms are like instructions, guiding AI on how to process and interpret data.**\\n* **They define the \\\"thinking process\\\" for the AI, determining how it analyzes information and reaches conclusions.**\\n* **Think of an algorithm as a recipe for a cake: the ingredients are the data, and the instructions are the algorithm.**\\n\\n**3. Machine Learning: The Core of AI's Power:**\\n\\n* **This is the ability of AI to \\\"learn\\\" on its own without explicit programming.**\\n* **Instead of being programmed with specific instructions, AI learns by analyzing data and identifying patterns.**\\n* **This allows AI to adapt and improve its performance over time, becoming more efficient and accurate.**\\n\\n**4. Different Types of AI:**\\n\\n* **Machine Learning:** This is the most common type, where AI learns from data to make predictions or classifications.\\n* **Deep Learning:** This is a subset of machine learning that uses complex neural networks inspired by the human brain, allowing for more sophisticated learning.\\n* **Natural Language Processing (NLP):** This focuses on enabling AI to understand and process human language, leading to things like voice assistants and chatbots.\\n* **Computer Vision:** This allows AI to \\\"see\\\" and interpret images and videos, leading to applications like facial recognition and self-driving cars.\\n\\n**5. Applications of AI in Our Lives:**\\n\\n* **Recommendation systems:**  Netflix, Spotify, and Amazon use AI to recommend movies, music, and products based on your preferences.\\n* **Spam filters:** AI analyzes emails and flags suspicious content to keep your inbox clean.\\n* **Self-driving cars:** AI analyzes real-time data from sensors to make driving decisions.\\n* **Medical diagnosis:** AI can assist doctors in identifying diseases from medical scans and patient data.\\n\\n**Key takeaway:** AI is not magic, but rather a powerful technology that learns from data and algorithms to perform complex tasks. It's constantly evolving and finding new ways to improve our lives, making it one of the most exciting fields in science and technology today.\\n\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0,\n",
      "      \"safetyRatings\": [\n",
      "        {\n",
      "          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "          \"probability\": \"NEGLIGIBLE\"\n",
      "        },\n",
      "        {\n",
      "          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "          \"probability\": \"NEGLIGIBLE\"\n",
      "        },\n",
      "        {\n",
      "          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "          \"probability\": \"NEGLIGIBLE\"\n",
      "        },\n",
      "        {\n",
      "          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "          \"probability\": \"NEGLIGIBLE\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 4,\n",
      "    \"candidatesTokenCount\": 569,\n",
      "    \"totalTokenCount\": 573\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verfify that our Google API key works\n",
    "\n",
    "import requests\n",
    "\n",
    "GOOGLE_API_KEY = os.environ['GOOGLE_API_KEY']\n",
    "url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={GOOGLE_API_KEY}'\n",
    "  \n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "  \n",
    "data = {\"contents\":[{\"parts\":[{\"text\":\"Explain how AI works\"}]}]}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the docs\n",
    "\n",
    "# TODO - add TextLoader refernece\n",
    "# TODO - add CSVLoader reference\n",
    "# https://python.langchain.com/docs/integrations/document_loaders/pymupdf/\n",
    "# https://python.langchain.com/docs/integrations/document_loaders/microsoft_word/\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "def process_file(path: str):\n",
    "\n",
    "    docs = None\n",
    "\n",
    "    # Select the right loader\n",
    "    if 'txt' in path.lower():\n",
    "        loader = TextLoader(path)\n",
    "    elif 'pdf' in path.lower():\n",
    "        loader = PyMuPDFLoader(path)\n",
    "    elif 'docx' in path.lower():\n",
    "        loader = Docx2txtLoader(path)\n",
    "    else:\n",
    "        print(f'No document loader found for {path}')\n",
    "\n",
    "    docs = loader.load()\n",
    "\n",
    "    return docs\n",
    "\n",
    "#####\n",
    "\n",
    "def test_process_file():\n",
    "    doc = process_file('docs/CHW_EOC_04-21-2017_ENG.pdf')\n",
    "    print(doc[0].metadata)\n",
    "\n",
    "test_process_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using Hugging Face \n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def create_embeddings_huggingface(model='Snowflake/snowflake-arctic-embed-m') -> HuggingFaceEmbeddings:\n",
    "\n",
    "    # Initialize the OpenAIEmbeddings class\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_embeddings_huggingface():\n",
    "    text = 'What is my benefit for acupuncture?'\n",
    "    embeddings = create_embeddings_huggingface()\n",
    "    vector = embeddings.embed_query(text)\n",
    "    print(vector)\n",
    "    return embeddings\n",
    "\n",
    "test_create_embeddings_huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using OpenAI\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def create_embeddings_openai(model='text-embedding-ada-002') -> OpenAIEmbeddings:\n",
    "\n",
    "    # Initialize the OpenAIEmbeddings class\n",
    "    embeddings = OpenAIEmbeddings(model=model)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_embeddings_openai():\n",
    "    text = 'What is my benefit for acupuncture?'\n",
    "    embeddings = create_embeddings_openai()\n",
    "    vector = embeddings.embed_query(text)\n",
    "    print(vector)\n",
    "    return embeddings\n",
    "\n",
    "test_create_embeddings_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using Vertex AI\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/text_embedding/google_vertex_ai_palm/\n",
    "\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "def create_embeddings_vertexai(model=\"text-embedding-004\") -> VertexAIEmbeddings:\n",
    "\n",
    "    # Initialize the VertexAIEmbeddings class\n",
    "    embeddings = VertexAIEmbeddings(model_name='text-embedding-005')\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_embeddings_vertexai():\n",
    "    text = 'What is my benefit for acupuncture?'\n",
    "    embeddings = create_embeddings_vertexai()\n",
    "    vector = embeddings.embed_query(text)\n",
    "    print(vector)\n",
    "    return embeddings\n",
    "\n",
    "test_create_embeddings_vertexai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty chunks \n",
    "\n",
    "def remove_empty_chunks(chunks_start: list) -> list:\n",
    "    \n",
    "    start = len(chunks_start)\n",
    "    # print(f'start - {start} chunks')\n",
    "    \n",
    "    # Remove empty chunks\n",
    "    chunks_end = [chunk for chunk in chunks_start if chunk.page_content.strip()]\n",
    "\n",
    "    end = len(chunks_end)\n",
    "    # print(f'end - {end} chunks')\n",
    "\n",
    "    return chunks_end   \n",
    "\n",
    "#####\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "def test_remove_empty_chunks():\n",
    "    docs = process_file('docs/CHW_EOC_04-21-2017_ENG.pdf')\n",
    "\n",
    "    # Added a test doc\n",
    "    doc = Document(\n",
    "        page_content='',\n",
    "        metadata=docs[0].metadata\n",
    "    )\n",
    "    docs.append(doc)\n",
    "    print(len(docs))\n",
    "\n",
    "    # Remove the empty doc (chunk)\n",
    "    docs = remove_empty_chunks(docs)\n",
    "    print(len(docs))\n",
    "\n",
    "test_remove_empty_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter using recursive character text splitter\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_docs_recursive(documents: list, chunk_size=500, chunk_overlap=50) -> list:\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    chunks_start = text_splitter.split_documents(documents)\n",
    "\n",
    "    chunks_end = remove_empty_chunks(chunks_start=chunks_start)\n",
    "\n",
    "    return chunks_end\n",
    "\n",
    "#####\n",
    "\n",
    "def test_chunk_docs_recursive(): \n",
    "    docs = process_file('docs/CHW_EOC_04-21-2017_ENG.pdf')\n",
    "    chunks = chunk_docs_recursive(documents=docs)\n",
    "    print(len(chunks))\n",
    "    print(chunks[0].page_content)\n",
    "\n",
    "test_chunk_docs_recursive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter using NLTK\n",
    "\n",
    "# https://python.langchain.com/docs/how_to/split_by_token/\n",
    "\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "def chunk_docs_nltk(documents: list, chunk_size=500, chunk_overlap=50) -> list:\n",
    "\n",
    "    text_splitter = NLTKTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap)\n",
    "\n",
    "    chunks_start = text_splitter.split_documents(documents)\n",
    "\n",
    "    chunks_end = remove_empty_chunks(chunks_start=chunks_start)\n",
    "\n",
    "    return chunks_end\n",
    "\n",
    "#####\n",
    "\n",
    "def test_chunk_docs_nltk(): \n",
    "    docs = process_file('docs/CHW_EOC_04-21-2017_ENG.pdf')\n",
    "    chunks = chunk_docs_nltk(documents=docs)\n",
    "    print(len(chunks))\n",
    "    print(chunks[0].page_content)\n",
    "\n",
    "test_chunk_docs_nltk()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter semantic chunking \n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/semantic-chunker/\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "def chunk_docs_semantic(documents: list, ) -> list:\n",
    "\n",
    "    # TODO - Use embeddings parameter\n",
    "    text_splitter = SemanticChunker(create_embeddings_openai(), breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "    chunks_start = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Remove empty chunks\n",
    "    chunks_end = remove_empty_chunks(chunks_start)\n",
    "\n",
    "    return chunks_end\n",
    "\n",
    "#####\n",
    "\n",
    "def test_chunk_docs_semantic():\n",
    "    docs = process_file('docs/CHW_EOC_04-21-2017_ENG.pdf')\n",
    "    chunks = chunk_docs_semantic(docs)\n",
    "    print(len(chunks))\n",
    "    print(chunks[0].page_content)\n",
    "\n",
    "test_chunk_docs_semantic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant vector store\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "def create_qdrant_vector_store(location: str, collection_name: str, vector_size: int, embeddings: Embeddings, documents: list) -> QdrantVectorStore:\n",
    "\n",
    "    # Initialize the Qdrant client\n",
    "    qdrant_client = QdrantClient(location=location)\n",
    "\n",
    "    # Create a collection in Qdrant\n",
    "    qdrant_client.create_collection(collection_name=collection_name, vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE))\n",
    "\n",
    "    # Initialize QdrantVectorStore with the Qdrant client\n",
    "    qdrant_vector_store = QdrantVectorStore(client=qdrant_client, collection_name=collection_name, embedding=embeddings)\n",
    "    \n",
    "    qdrant_vector_store.add_documents(documents)\n",
    "    \n",
    "    return qdrant_vector_store\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_qdrant_vector_store():\n",
    "    embeddings = create_embeddings_openai()\n",
    "    docs = process_file('docs/CHW_EOC_04-21-2017_ENG.pdf')\n",
    "    print(len(docs))\n",
    "    chunks = chunk_docs_recursive(docs)\n",
    "    print(len(chunks))\n",
    "    vector_store = create_qdrant_vector_store(\":memory:\", \"test\", 1536, embeddings, chunks)\n",
    "    print(vector_store.collection_name)\n",
    "\n",
    "test_create_qdrant_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant retriever\n",
    "\n",
    "# TODO - Add reference \n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "def create_retriever_qdrant(vector_store: QdrantVectorStore) -> BaseRetriever:\n",
    "\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    return retriever\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_retriever_qdrant(text: str = None):\n",
    "    embeddings = create_embeddings_openai()\n",
    "    docs = process_file('docs/CHW_EOC_04-21-2017_ENG.pdf')\n",
    "    chunks = chunk_docs_recursive(docs)\n",
    "    vector_store = create_qdrant_vector_store(\":memory:\", \"test\", 1536, embeddings, chunks)\n",
    "    retriever = create_retriever_qdrant(vector_store)\n",
    "    if text:\n",
    "        docs = retriever.invoke(text)\n",
    "        print(docs[0])\n",
    "\n",
    "print('\\nQDRANT')\n",
    "test_create_retriever_qdrant('What is my benefit for acupuncture?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Vertex AI retriever\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/retrievers/google_vertex_ai_search/\n",
    "\n",
    "from langchain_google_community import VertexAISearchRetriever\n",
    "\n",
    "def create_retriever_vertexai() -> VertexAISearchRetriever:\n",
    "\n",
    "    retriever = VertexAISearchRetriever(project_id=os.environ['PROJECT_ID'], location_id=os.environ['LOCATION_ID'], data_store_id=os.environ['DATA_STORE_ID'], max_documents=3)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_retriever_vertexai(text: str = None):\n",
    "    retriever = create_retriever_vertexai()\n",
    "    if text:\n",
    "        docs = retriever.invoke(text)\n",
    "        print(docs[0])\n",
    "\n",
    "print('\\nVERTEX AI')\n",
    "test_create_retriever_vertexai('What is my benefit for acupuncture?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/#chatprompttemplate\n",
    "# https://python.langchain.com/v0.2/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def create_chat_prompt_template(prompt: str = None) -> ChatPromptTemplate:\n",
    "    \n",
    "    template = '''\n",
    "    You are a helpful conversational agent for the State of California.\n",
    "    Your expertise is fully understanding the California Health & Wellness health  plan. \n",
    "    You need to answer questions posed by the member, who is trying to get answers about their health plan.  \n",
    "    Your goal is to provide a helpful and detailed response, in at least 2-3 sentences. \n",
    "\n",
    "    You will be analyzing the health plan documents to derive a good answer, based on the following information:\n",
    "    1. The question asked.\n",
    "    2. The provided context, which comes from various documents of the pharmacy manuals repository. You will need to answer the question based on the provided context.\n",
    "\n",
    "    Now it's your turn!\n",
    "\n",
    "    {question}\n",
    "\n",
    "    {context}\n",
    "\n",
    "    '''\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_chat_prompt_template():\n",
    "    prompt = create_chat_prompt_template()\n",
    "    print(prompt)\n",
    "\n",
    "test_create_chat_prompt_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Langchain chain..\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/llms/google_ai/\n",
    "# https://python.langchain.com/docs/integrations/chat/google_generative_ai/\n",
    "# https://ai.google.dev/gemini-api/docs/safety-settings \n",
    "\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from operator import itemgetter\n",
    "\n",
    "def create_chain (model_name: str, prompt: ChatPromptTemplate, retriever: BaseRetriever):\n",
    "\n",
    "    if \"gemini\" in model_name.lower():\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=model_name,\n",
    "            temperature=0,\n",
    "            safety_settings={\n",
    "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                },\n",
    "            )\n",
    "    else:\n",
    "        print(\"Unsuported model name\")\n",
    "        \n",
    "    chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")} \n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\")) \n",
    "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "        )\n",
    "\n",
    "    return chain\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_chain_qdrant():\n",
    "    embeddings = create_embeddings_openai()\n",
    "    docs = process_file('docs/CHW_EOC_04-21-2017_ENG.pdf')\n",
    "    chunks = chunk_docs_recursive(docs)\n",
    "    vector_store = create_qdrant_vector_store(\":memory:\", \"test\", 1536, embeddings, chunks)\n",
    "    retriever = create_retriever_qdrant(vector_store)\n",
    "    chat_prompt_template = create_chat_prompt_template()\n",
    "    chain = create_chain('gemini-1.5-flash', chat_prompt_template, retriever)\n",
    "    result = chain.invoke({'question' : 'What is my benefit for acupuncture?'})\n",
    "    print(result)\n",
    "\n",
    "print('\\nQDRANT')\n",
    "test_create_chain_qdrant()\n",
    "\n",
    "###\n",
    "\n",
    "def test_create_chain_vertexai():\n",
    "    retreiver = create_retriever_vertexai()\n",
    "    chat_prompt_template = create_chat_prompt_template()\n",
    "    chain = create_chain('gemini-1.5-flash', chat_prompt_template, retreiver)\n",
    "    result = chain.invoke({'question' : 'What is my benefit for acupuncture?'})\n",
    "    print(result)\n",
    "\n",
    "print('\\nVERTEX AI')\n",
    "test_create_chain_vertexai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answeers from a chain usin a list of questions\n",
    "\n",
    "def generate_answers_contexts(chain, questions: list):\n",
    "    \n",
    "    answers = []\n",
    "    contexts = []\n",
    "\n",
    "    # Loop over the list of questions and call the chain to get the answer and context\n",
    "    for question in questions:\n",
    "        print(question)\n",
    "\n",
    "        # Call the chain to get answers and contexts\n",
    "        response = chain.invoke({\"question\" : question})\n",
    "        print(response)\n",
    "        \n",
    "        # Capture the answer and context \n",
    "        answers.append(response[\"response\"].content)\n",
    "        contexts.append([context.page_content for context in response[\"context\"]])\n",
    "\n",
    "    return answers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Ragas evaluation \n",
    "\n",
    "from datasets import Dataset\n",
    "from pandas import DataFrame\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (faithfulness, answer_relevancy, answer_correctness, context_recall, context_precision)\n",
    "\n",
    "def run_ragas_evaluation(chain, questions: list, groundtruths: list, eval_metrics: list = [answer_correctness, answer_relevancy, context_recall, context_precision, faithfulness]):\n",
    "\n",
    "  answers = []\n",
    "  contexts = []\n",
    "  answers, contexts = generate_answers_contexts(chain, questions)\n",
    "\n",
    "  # Create the input dataset \n",
    "  input_dataset = Dataset.from_dict({\n",
    "  \"question\" : questions,         # From the dataframe\n",
    "  \"answer\" : answers,             # From the chain\n",
    "  \"contexts\" : contexts,          # From the chain\n",
    "  \"ground_truth\" : groundtruths   # From the dataframe\n",
    "  })\n",
    "\n",
    "  # Run the Ragas evaluation using the input dataset and eval metrics\n",
    "  ragas_results = evaluate(input_dataset, eval_metrics)\n",
    "  ragas_results_df = ragas_results.to_pandas()\n",
    "  \n",
    "  return ragas_results, ragas_results_df\n",
    "  \n",
    "  #####\n",
    "\n",
    "def test_run_ragas_evaluation():\n",
    "  print(\"test\")    \n",
    "\n",
    "test_run_ragas_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Vertex AI Search datastore using HTTP Post\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import google.auth\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "credentials, project_id = google.auth.default()\n",
    "credentials.refresh(Request())\n",
    "access_token = credentials.token\n",
    "print(access_token)\n",
    "\n",
    "def query_chunks(query, n=5):\n",
    "    \n",
    "  if LOCATION_ID == 'us':\n",
    "    api_endpoint = 'us-discoveryengine.googleapis.com'\n",
    "  else:\n",
    "    api_endpoint = 'discoveryengine.googleapis.com'\n",
    "\n",
    "  url = f\"https://{api_endpoint}/v1alpha/projects/{PROJECT_ID}/locations/{LOCATION_ID}/collections/default_collection/dataStores/{DATA_STORE_ID}/servingConfigs/default_search:search\"\n",
    "  print(url)\n",
    "  \n",
    "  headers = {\n",
    "      \"Authorization\": f\"Bearer {access_token}\",\n",
    "      \"Content-Type\": \"application/json\",\n",
    "  }\n",
    "  \n",
    "  post_data = {\n",
    "      \"servingConfig\": f\"projects/{PROJECT_ID}/locations/{LOCATION_ID}/collections/default_collection/dataStores/{DATA_STORE_ID}/servingConfigs/default_search\",\n",
    "      \"pageSize\": n,\n",
    "      \"query\": query,\n",
    "      \"contentSearchSpec\": {\"searchResultMode\": \"CHUNKS\"},\n",
    "  }\n",
    "  \n",
    "  response = requests.post(url, headers=headers, json=post_data)\n",
    "\n",
    "  if response.status_code != 200:\n",
    "    print(\n",
    "        f\"Error retrieving search results: {response.status_code} -\"\n",
    "        f\" {response.text}\"\n",
    "    )\n",
    "\n",
    "  return response.json()\n",
    "\n",
    "#####\n",
    "\n",
    "test = query_chunks('What is my benefit for acupuncture?')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RAG chain using Vertex AI vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RAG chain using Vertex AI Agent Builder datastore\n",
    "\n",
    "retreiver = create_retriever_vertexai()\n",
    "chat_prompt_template = create_chat_prompt_template()\n",
    "chain = create_chain('gemini-1.5-flash', chat_prompt_template, retreiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the chain \n",
    "\n",
    "questions = ['What is my benefit for acupuncture?',\n",
    "'Who should I call if I have an emergency?',\n",
    "'What are the responsibilities of my PCP?',]\n",
    "\n",
    "for question in questions:\n",
    "    print(question)\n",
    "    result = chain.invoke({\"question\" : question})\n",
    "    print(result)\n",
    "    print(result[\"response\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate answers from the golden Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Golden Q&A\n",
    "testset_df = pd.read_excel('golden_qa/KN Virtual Assist POC_08.09.24 1_mk.xlsx', 'Consolidated Golden QnA')\n",
    "\n",
    "questions = testset_df[\"Question\"].values.tolist()\n",
    "questions = [str(question) for question in questions]\n",
    "\n",
    "answers, contexts = generate_answers_contexts(chain, questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create customer testset and evaluate using Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the golden Q&A and get questions and ground truths\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "testset_df = pd.read_excel('golden_qa/KN Virtual Assist POC_08.09.24 1_mk.xlsx', 'Consolidated Golden QnA')\n",
    "\n",
    "questions = testset_df[\"Question\"].values.tolist()\n",
    "questions = [str(question) for question in questions]\n",
    "\n",
    "groundtruths = testset_df[\"Answer\"].values.tolist()\n",
    "groundtruths = [str(ground_truth) for ground_truth in groundtruths]\n",
    "\n",
    "print(\"Writing customer_testset.csv\")\n",
    "testset_df.to_csv(\"testsets/customer_testset.csv\")\n",
    "testset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the customer testset using Ragas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Get the questions and groundtruths from the dataframe\n",
    "testset_df = pd.read_csv(\"testsets/customer_testset.csv\")\n",
    "\n",
    "questions = testset_df[\"Question\"].values.tolist()\n",
    "questions = [str(question) for question in questions]\n",
    "\n",
    "groundtruths = testset_df[\"Answer\"].values.tolist()\n",
    "groundtruths = [str(ground_truth) for ground_truth in groundtruths]  \n",
    "\n",
    "# Specify the eval metrics\n",
    "eval_metrics = [answer_correctness, answer_relevancy, context_precision, context_recall, faithfulness]\n",
    "\n",
    "# Run the Ragas evaluation and show the results\n",
    "ragas_results, ragas_results_df = run_ragas_evaluation(chain, questions, groundtruths, eval_metrics)\n",
    "\n",
    "# Write the results to disk\n",
    "print(\"Writing customer_testset_ragas_results.csv\")\n",
    "ragas_results_df.to_csv(\"ragas/customer_testset_ragas_results.csv\")\n",
    "\n",
    "# Show the resutls\n",
    "ragas_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create synthetic testset and evaluate using Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "\n",
    "# Load the docs\n",
    "documents = []\n",
    "\n",
    "paths = ['docs/CHW_EOC_04-21-2017_ENG.pdf']\n",
    "\n",
    "for path in paths:\n",
    "    documents.extend(process_file(path=path))\n",
    "\n",
    "# Chunk the docs \n",
    "# chunks = chunk_docs_nltk(documents, 1500, 150)\n",
    "chunks = chunk_docs_semantic(documents)\n",
    "\n",
    "# Set up the parameters for generating the testset\n",
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "generator = TestsetGenerator.from_langchain(generator_llm, critic_llm, embeddings)\n",
    "distributions = {simple: 0.5, multi_context: 0.4, reasoning: 0.1}\n",
    "\n",
    "# Generate the testset and save to disk \n",
    "testset = generator.generate_with_langchain_docs(documents=chunks, test_size=50, distributions=distributions)\n",
    "testset_df = testset.to_pandas()\n",
    "\n",
    "print(\"Writing synthetic_testset.csv\")\n",
    "testset_df.to_csv(\"testsets/synthetic_testset.csv\")\n",
    "testset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the synthetic testset using Ragas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Run the Ragas evaluation and show the results\n",
    "# Get the questions and groundtruths from the dataframe\n",
    "testset_df = pd.read_csv(\"testsets/synthetic_testset.csv\")\n",
    "\n",
    "questions = testset_df[\"question\"].values.tolist()\n",
    "questions = [str(question) for question in questions]\n",
    "\n",
    "groundtruths = testset_df[\"ground_truth\"].values.tolist()\n",
    "groundtruths = [str(ground_truth) for ground_truth in groundtruths]  \n",
    "\n",
    "# Specify the eval metrics\n",
    "eval_metrics = [answer_correctness, answer_relevancy, context_precision, context_recall, faithfulness]\n",
    "\n",
    "# Run the Ragas evaluation and show the results\n",
    "ragas_results, ragas_results_df = run_ragas_evaluation(chain, questions, groundtruths, eval_metrics)\n",
    "\n",
    "# Write the results to disk\n",
    "print(\"Writing synthetic_testset_ragas_results.csv\")\n",
    "ragas_results_df.to_csv(\"ragas/synthetic_testset_ragas_results.csv\")\n",
    "\n",
    "# Show the resutls\n",
    "ragas_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sample ReAct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729533778.578926 2869469 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: I need to find the paper on arXiv.org.\n",
      "Action: arxiv\n",
      "Action Input: 1605.08386\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2016-05-26\n",
      "Title: Heat-bath random walks with Markov bases\n",
      "Authors: Caprice Stanley, Tobias Windisch\n",
      "Summary: Graphs on lattice points are studied whose edges come from a finite set of\n",
      "allowed moves of arbitrary length. We show that the diameter of these graphs on\n",
      "fibers of a fixed integer matrix can be bounded from above by a constant. We\n",
      "then study the mixing behaviour of heat-bath random walks on these graphs. We\n",
      "also state explicit conditions on the set of moves so that the heat-bath random\n",
      "walk, a generalization of the Glauber dynamics, is an expander in fixed\n",
      "dimension.\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "Final Answer: The paper \"Heat-bath random walks with Markov bases\" by Caprice Stanley and Tobias Windisch studies graphs on lattice points with edges defined by a finite set of allowed moves. It focuses on bounding the diameter of these graphs on fibers of a fixed integer matrix and analyzing the mixing behavior of heat-bath random walks on them. The paper also provides conditions for the heat-bath random walk to be an expander in fixed dimension. \n",
      " of heat-bath random walks on them. The paper also provides conditions for the heat-bath random walk to be an expander in fixed dimension. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What's the paper 1605.08386 about?\",\n",
       " 'output': 'The paper \"Heat-bath random walks with Markov bases\" by Caprice Stanley and Tobias Windisch studies graphs on lattice points with edges defined by a finite set of allowed moves. It focuses on bounding the diameter of these graphs on fibers of a fixed integer matrix and analyzing the mixing behavior of heat-bath random walks on them. The paper also provides conditions for the heat-bath random walk to be an expander in fixed dimension. \\n of heat-bath random walks on them. The paper also provides conditions for the heat-bath random walk to be an expander in fixed dimension.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            temperature=0,\n",
    "            safety_settings={\n",
    "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                },\n",
    "            )\n",
    "\n",
    "tools = load_tools([\"arxiv\"])\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, \n",
    "                           tools, \n",
    "                           prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, \n",
    "                               tools=tools, \n",
    "                               verbose=True)\n",
    "\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What's the paper 1605.08386 about?\",\n",
    "        }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aie4-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
