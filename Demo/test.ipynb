{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "\n",
    "%pip install -qU langchain-cohere langchain-community langchain-core==0.2.40 langchain_experimental langchain-google-community langchain-google-genai langchain-huggingface==0.0.3 langchain-openai langchain-qdrant langchain-google-vertexai\n",
    "%pip install -qU -q chainlit==1.1.302\n",
    "%pip install -qU -q docx2txt\n",
    "%pip install -qU google-cloud-aiplatform\n",
    "%pip install -qU google-cloud-discoveryengine\n",
    "%pip install -qU nltk\n",
    "%pip install -qU openpyxl\n",
    "%pip install -qU pymupdf\n",
    "%pip install -qU python-dotenv\n",
    "%pip install -qU ragas==0.1.20\n",
    "%pip install -qU tqdm\n",
    "%pip install -qU unstructured \n",
    "\n",
    "# Verify installed packages have compatible dependencies\n",
    "%pip check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download punkt_tab module that is used for sentence tokenizaiton\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get environment variables\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Google user permissions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If running in Colab, use the permissions of the currently authenticated user\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('Running in Google Colab')\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()\n",
    "\n",
    "# If not, set the GOOGLE_APPLICATION_CREDENTIALS to the service account credentials file \n",
    "else:\n",
    "    print('Running locally')\n",
    "    # os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'credentials.json'\n",
    "    # print(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
    "\n",
    "#####\n",
    "\n",
    "def test_google_perms():\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # Now, you can use the Google Cloud client libraries\n",
    "    client = storage.Client()\n",
    "\n",
    "    # List all buckets in your project\n",
    "    buckets = list(client.list_buckets())\n",
    "    print(buckets)\n",
    "\n",
    "test_google_perms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=os.environ['PROJECT_ID'], location=os.environ['REGION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verfify that our Google API key works\n",
    "\n",
    "import requests\n",
    "\n",
    "GOOGLE_API_KEY = os.environ['GOOGLE_API_KEY']\n",
    "url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={GOOGLE_API_KEY}'\n",
    "  \n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "  \n",
    "data = {\"contents\":[{\"parts\":[{\"text\":\"Explain how AI works\"}]}]}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the docs from a directory\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/file_directory/\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader \n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "\n",
    "def process_directory(path: str, glob: str, loader_cls: str, use_multithreading=True):\n",
    "    \n",
    "    loader = DirectoryLoader(path=path, glob=glob, show_progress=True, loader_cls=loader_cls, use_multithreading=use_multithreading)\n",
    "    \n",
    "    docs = loader.load()\n",
    "    \n",
    "    return docs\n",
    "\n",
    "#####\n",
    "\n",
    "def test_process_directory():\n",
    "    docs = []\n",
    "    \n",
    "    docs_pdf = process_directory('docs/medi-cal/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "    # print(len(docs_pdf))\n",
    "    docs_excel = process_directory('docs/medi-cal/', '**/*.xlsx', UnstructuredExcelLoader, True)\n",
    "    # print(len(docs_excel))\n",
    "\n",
    "    docs.extend(docs_pdf)\n",
    "    docs.extend(docs_excel)\n",
    "    print(len(docs))\n",
    "\n",
    "test_process_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the docs from a single file\n",
    "\n",
    "# TODO - add TextLoader refernece\n",
    "# TODO - add CSVLoader reference\n",
    "# https://python.langchain.com/docs/integrations/document_loaders/pymupdf/\n",
    "# https://python.langchain.com/docs/integrations/document_loaders/microsoft_word/\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "def process_file(path: str):\n",
    "\n",
    "    docs = None\n",
    "\n",
    "    # Select the right loader\n",
    "    if 'txt' in path.lower():\n",
    "        loader = TextLoader(path)\n",
    "    elif 'pdf' in path.lower():\n",
    "        loader = PyMuPDFLoader(path)\n",
    "    elif 'docx' in path.lower():\n",
    "        loader = Docx2txtLoader(path)\n",
    "    else:\n",
    "        print(f'No document loader found for {path}')\n",
    "\n",
    "    docs = loader.load()\n",
    "\n",
    "    return docs\n",
    "\n",
    "#####\n",
    "\n",
    "def test_process_file():\n",
    "    doc = process_file('docs/medi-cal/acu.pdf')\n",
    "    print(doc[0].metadata)\n",
    "\n",
    "test_process_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using Hugging Face \n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def create_embeddings_huggingface(model='Snowflake/snowflake-arctic-embed-m') -> HuggingFaceEmbeddings:\n",
    "\n",
    "    # Initialize the OpenAIEmbeddings class\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_embeddings_huggingface():\n",
    "    text = 'What is my benefit for acupuncture?'\n",
    "    embeddings = create_embeddings_huggingface()\n",
    "    vector = embeddings.embed_query(text)\n",
    "    print(vector)\n",
    "    return embeddings\n",
    "\n",
    "test_create_embeddings_huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using OpenAI\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def create_embeddings_openai(model='text-embedding-ada-002') -> OpenAIEmbeddings:\n",
    "\n",
    "    # Initialize the OpenAIEmbeddings class\n",
    "    embeddings = OpenAIEmbeddings(model=model)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_embeddings_openai():\n",
    "    text = 'What is my benefit for acupuncture?'\n",
    "    embeddings = create_embeddings_openai()\n",
    "    vector = embeddings.embed_query(text)\n",
    "    print(vector)\n",
    "    return embeddings\n",
    "\n",
    "test_create_embeddings_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using Vertex AI\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/text_embedding/google_vertex_ai_palm/\n",
    "\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "def create_embeddings_vertexai(model=\"text-embedding-004\") -> VertexAIEmbeddings:\n",
    "\n",
    "    # Initialize the VertexAIEmbeddings class\n",
    "    embeddings = VertexAIEmbeddings(model_name=model)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_embeddings_vertexai():\n",
    "    text = 'What is my benefit for acupuncture?'\n",
    "    embeddings = create_embeddings_vertexai()\n",
    "    vector = embeddings.embed_query(text)\n",
    "    print(vector)\n",
    "    return embeddings\n",
    "\n",
    "test_create_embeddings_vertexai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty chunks \n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "def remove_empty_chunks(chunks_start: list) -> list:\n",
    "    \n",
    "    start = len(chunks_start)\n",
    "    # print(f'start - {start} chunks')\n",
    "    \n",
    "    # Remove empty chunks\n",
    "    chunks_end = [chunk for chunk in chunks_start if chunk.page_content.strip()]\n",
    "\n",
    "    end = len(chunks_end)\n",
    "    # print(f'end - {end} chunks')\n",
    "\n",
    "    return chunks_end   \n",
    "\n",
    "#####\n",
    "\n",
    "def test_remove_empty_chunks():\n",
    "    docs = process_file('docs/medi-cal/acu.pdf')\n",
    "\n",
    "    # Added a test doc\n",
    "    doc = Document(\n",
    "        page_content='',\n",
    "        metadata=docs[0].metadata\n",
    "    )\n",
    "    docs.append(doc)\n",
    "    print(len(docs))\n",
    "\n",
    "    # Remove the empty doc (chunk)\n",
    "    docs = remove_empty_chunks(docs)\n",
    "    print(len(docs))\n",
    "\n",
    "test_remove_empty_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter using recursive character text splitter\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_docs_recursive(documents: list, chunk_size=500, chunk_overlap=50) -> list:\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    chunks_start = text_splitter.split_documents(documents)\n",
    "\n",
    "    chunks_end = remove_empty_chunks(chunks_start=chunks_start)\n",
    "\n",
    "    return chunks_end\n",
    "\n",
    "#####\n",
    "\n",
    "def test_chunk_docs_recursive(): \n",
    "    docs = process_file('docs/medi-cal/acu.pdf')\n",
    "    chunks = chunk_docs_recursive(documents=docs)\n",
    "    print(len(chunks))\n",
    "    print(chunks[0].page_content)\n",
    "\n",
    "test_chunk_docs_recursive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter using NLTK\n",
    "\n",
    "# https://python.langchain.com/docs/how_to/split_by_token/\n",
    "\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "def chunk_docs_nltk(documents: list, chunk_size=500, chunk_overlap=50) -> list:\n",
    "\n",
    "    text_splitter = NLTKTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap)\n",
    "\n",
    "    chunks_start = text_splitter.split_documents(documents)\n",
    "\n",
    "    chunks_end = remove_empty_chunks(chunks_start=chunks_start)\n",
    "\n",
    "    return chunks_end\n",
    "\n",
    "#####\n",
    "\n",
    "def test_chunk_docs_nltk(): \n",
    "    docs = process_file('docs/medi-cal/acu.pdf')\n",
    "    chunks = chunk_docs_nltk(documents=docs)\n",
    "    print(len(chunks))\n",
    "    print(chunks[0].page_content)\n",
    "\n",
    "test_chunk_docs_nltk()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter semantic chunking \n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/semantic-chunker/\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "def chunk_docs_semantic(documents: list, ) -> list:\n",
    "\n",
    "    # TODO - Use embeddings parameter\n",
    "    text_splitter = SemanticChunker(create_embeddings_openai(), breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "    chunks_start = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Remove empty chunks\n",
    "    chunks_end = remove_empty_chunks(chunks_start)\n",
    "\n",
    "    return chunks_end\n",
    "\n",
    "#####\n",
    "\n",
    "def test_chunk_docs_semantic():\n",
    "    docs = process_file('docs/medi-cal/acu.pdf')\n",
    "    chunks = chunk_docs_semantic(docs)\n",
    "    print(len(chunks))\n",
    "    print(chunks[0].page_content)\n",
    "\n",
    "test_chunk_docs_semantic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant vector store\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "def create_qdrant_vector_store(location: str, collection_name: str, vector_size: int, embeddings: Embeddings, documents: list) -> QdrantVectorStore:\n",
    "\n",
    "    # Initialize the Qdrant client\n",
    "    qdrant_client = QdrantClient(location=location)\n",
    "\n",
    "    # Create a collection in Qdrant\n",
    "    qdrant_client.create_collection(collection_name=collection_name, vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE))\n",
    "\n",
    "    # Initialize QdrantVectorStore with the Qdrant client\n",
    "    qdrant_vector_store = QdrantVectorStore(client=qdrant_client, collection_name=collection_name, embedding=embeddings)\n",
    "    \n",
    "    qdrant_vector_store.add_documents(documents)\n",
    "    \n",
    "    return qdrant_vector_store\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_qdrant_vector_store():\n",
    "    embeddings = create_embeddings_openai()\n",
    "    docs = process_file('docs/medi-cal/acu.pdf')\n",
    "    print(len(docs))\n",
    "    chunks = chunk_docs_recursive(docs)\n",
    "    print(len(chunks))\n",
    "    vector_store = create_qdrant_vector_store(\":memory:\", \"test\", 1536, embeddings, chunks)\n",
    "    print(vector_store.collection_name)\n",
    "\n",
    "test_create_qdrant_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant retriever\n",
    "\n",
    "# TODO - Add reference \n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "def create_retriever_qdrant(vector_store: QdrantVectorStore) -> BaseRetriever:\n",
    "\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    return retriever\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_retriever_qdrant(text: str = None):\n",
    "    embeddings = create_embeddings_openai()\n",
    "    docs = process_file('docs/medi-cal/acu.pdf')\n",
    "    chunks = chunk_docs_recursive(docs)\n",
    "    vector_store = create_qdrant_vector_store(\":memory:\", \"test\", 1536, embeddings, chunks)\n",
    "    retriever = create_retriever_qdrant(vector_store)\n",
    "    if text:\n",
    "        docs = retriever.invoke(text)\n",
    "        print(docs[0])\n",
    "\n",
    "print('\\nQDRANT')\n",
    "test_create_retriever_qdrant('What is my benefit for acupuncture?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Vertex AI retriever\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/retrievers/google_vertex_ai_search/\n",
    "\n",
    "from langchain_google_community import VertexAISearchRetriever\n",
    "\n",
    "def create_retriever_vertexai() -> VertexAISearchRetriever:\n",
    "\n",
    "    retriever = VertexAISearchRetriever(project_id=os.environ['PROJECT_ID'], location_id=os.environ['LOCATION_ID'], data_store_id=os.environ['DATA_STORE_ID'], max_documents=3)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_retriever_vertexai(text: str = None):\n",
    "    retriever = create_retriever_vertexai()\n",
    "    if text:\n",
    "        docs = retriever.invoke(text)\n",
    "        print(docs[0])\n",
    "\n",
    "print('\\nVERTEX AI')\n",
    "test_create_retriever_vertexai('What is my benefit for acupuncture?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/#chatprompttemplate\n",
    "# https://python.langchain.com/v0.2/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def create_chat_prompt_template(prompt: str = None) -> ChatPromptTemplate:\n",
    "    \n",
    "    template = '''\n",
    "    You are a helpful conversational agent for the State of California.\n",
    "    Your expertise is fully understanding the Medi-Cal provider manuals. \n",
    "    You need to answer questions posed by a member, who is trying to get answers about services provided by Medi-Cal.  \n",
    "    Your goal is to provide a helpful and detailed response, in at least 2-3 sentences. \n",
    "\n",
    "    You will be analyzing the health plan documents to derive a good answer, based on the following information:\n",
    "    1. The question asked.\n",
    "    2. The provided context, which comes from health plan document. You will need to answer the question based on the provided context and the conversation history.\n",
    "\n",
    "    Now it's your turn!\n",
    "\n",
    "    {question}\n",
    "\n",
    "    {context}\n",
    "    '''\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_chat_prompt_template():\n",
    "    prompt = create_chat_prompt_template()\n",
    "    print(prompt)\n",
    "\n",
    "test_create_chat_prompt_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Langchain chain..\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/llms/google_ai/\n",
    "# https://python.langchain.com/docs/integrations/chat/google_generative_ai/\n",
    "# https://ai.google.dev/gemini-api/docs/safety-settings \n",
    "\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from operator import itemgetter\n",
    "\n",
    "def create_chain (model_name: str, prompt: ChatPromptTemplate, retriever: BaseRetriever):\n",
    "\n",
    "    if \"gemini\" in model_name.lower():\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=model_name,\n",
    "            temperature=0,\n",
    "            safety_settings={\n",
    "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                },\n",
    "            )\n",
    "    else:\n",
    "        print(\"Unsuported model name\")\n",
    "        \n",
    "    chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")} \n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\")) \n",
    "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "        )\n",
    "\n",
    "    return chain\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_chain_qdrant():\n",
    "    embeddings = create_embeddings_openai()\n",
    "    docs = process_file('docs/medi-cal/acu.pdf')\n",
    "    chunks = chunk_docs_recursive(docs)\n",
    "    vector_store = create_qdrant_vector_store(\":memory:\", \"test\", 1536, embeddings, chunks)\n",
    "    retriever = create_retriever_qdrant(vector_store)\n",
    "    chat_prompt_template = create_chat_prompt_template()\n",
    "    chain = create_chain('gemini-1.5-flash', chat_prompt_template, retriever)\n",
    "    result = chain.invoke({'question' : 'What is my benefit for acupuncture?'})\n",
    "    print(result)\n",
    "\n",
    "print('\\nQDRANT')\n",
    "test_create_chain_qdrant()\n",
    "\n",
    "###\n",
    "\n",
    "def test_create_chain_vertexai():\n",
    "    retreiver = create_retriever_vertexai()\n",
    "    chat_prompt_template = create_chat_prompt_template()\n",
    "    chain = create_chain('gemini-1.5-flash', chat_prompt_template, retreiver)\n",
    "    result = chain.invoke({'question' : 'What is my benefit for acupuncture?'})\n",
    "    print(result)\n",
    "\n",
    "print('\\nVERTEX AI')\n",
    "test_create_chain_vertexai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers from a chain usin a list of questions\n",
    "\n",
    "def generate_answers_contexts(chain, questions: list):\n",
    "    \n",
    "    answers = []\n",
    "    contexts = []\n",
    "\n",
    "    # Loop over the list of questions and call the chain to get the answer and context\n",
    "    for question in questions:\n",
    "        print(question)\n",
    "\n",
    "        # Call the chain to get answers and contexts\n",
    "        response = chain.invoke({\"question\" : question})\n",
    "        print(response)\n",
    "        \n",
    "        # Capture the answer and context \n",
    "        answers.append(response[\"response\"].content)\n",
    "        contexts.append([context.page_content for context in response[\"context\"]])\n",
    "\n",
    "    return answers, contexts\n",
    "\n",
    "#####\n",
    "\n",
    "# TODO - Add test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Ragas evaluation \n",
    "\n",
    "from datasets import Dataset\n",
    "from pandas import DataFrame\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (faithfulness, answer_relevancy, answer_correctness, context_recall, context_precision)\n",
    "\n",
    "def run_ragas_evaluation(chain, questions: list, groundtruths: list, eval_metrics: list = [answer_correctness, answer_relevancy, context_recall, context_precision, faithfulness]):\n",
    "\n",
    "  answers = []\n",
    "  contexts = []\n",
    "  answers, contexts = generate_answers_contexts(chain, questions)\n",
    "\n",
    "  # Create the input dataset \n",
    "  input_dataset = Dataset.from_dict({\n",
    "  \"question\" : questions,         # From the dataframe\n",
    "  \"answer\" : answers,             # From the chain\n",
    "  \"contexts\" : contexts,          # From the chain\n",
    "  \"ground_truth\" : groundtruths   # From the dataframe\n",
    "  })\n",
    "\n",
    "  # Run the Ragas evaluation using the input dataset and eval metrics\n",
    "  ragas_results = evaluate(input_dataset, eval_metrics)\n",
    "  ragas_results_df = ragas_results.to_pandas()\n",
    "  \n",
    "  return ragas_results, ragas_results_df\n",
    "  \n",
    "#####\n",
    "\n",
    "def test_run_ragas_evaluation():\n",
    "  print(\"test\")    \n",
    "\n",
    "test_run_ragas_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Vertex AI Search datastore using HTTP Post\n",
    "\n",
    "import requests\n",
    "import google.auth\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "credentials, project_id = google.auth.default()\n",
    "credentials.refresh(Request())\n",
    "access_token = credentials.token\n",
    "print(access_token)\n",
    "\n",
    "def query_chunks(query, n=5):\n",
    "    \n",
    "  PROJECT_ID = os.environ['PROJECT_ID']\n",
    "  LOCATION_ID = os.environ['LOCATION_ID']\n",
    "  DATA_STORE_ID = os.environ['DATA_STORE_ID']\n",
    "\n",
    "  if LOCATION_ID == 'us':\n",
    "    api_endpoint = 'us-discoveryengine.googleapis.com'\n",
    "  else:\n",
    "    api_endpoint = 'discoveryengine.googleapis.com'\n",
    "\n",
    "  url = f\"https://{api_endpoint}/v1alpha/projects/{PROJECT_ID}/locations/{LOCATION_ID}/collections/default_collection/dataStores/{DATA_STORE_ID}/servingConfigs/default_search:search\"\n",
    "  print(url)\n",
    "  \n",
    "  headers = {\n",
    "      \"Authorization\": f\"Bearer {access_token}\",\n",
    "      \"Content-Type\": \"application/json\",\n",
    "  }\n",
    "  \n",
    "  post_data = {\n",
    "      \"servingConfig\": f\"projects/{PROJECT_ID}/locations/{LOCATION_ID}/collections/default_collection/dataStores/{DATA_STORE_ID}/servingConfigs/default_search\",\n",
    "      \"pageSize\": n,\n",
    "      \"query\": query,\n",
    "      \"contentSearchSpec\": {\"searchResultMode\": \"CHUNKS\"},\n",
    "  }\n",
    "  \n",
    "  response = requests.post(url, headers=headers, json=post_data)\n",
    "\n",
    "  if response.status_code != 200:\n",
    "    print(\n",
    "        f\"Error retrieving search results: {response.status_code} -\"\n",
    "        f\" {response.text}\"\n",
    "    )\n",
    "\n",
    "  return response.json()\n",
    "\n",
    "#####\n",
    "\n",
    "test = query_chunks('What is my benefit for acupuncture?')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RAG chain using Google Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RAG chain using Vertex AI Agent Builder datastore\n",
    "\n",
    "retreiver = create_retriever_vertexai()\n",
    "chat_prompt_template = create_chat_prompt_template()\n",
    "chain = create_chain('gemini-1.5-flash', chat_prompt_template, retreiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the chain \n",
    "\n",
    "questions = ['What is my benefit for acupuncture?',\n",
    "'Who should I call if I have an emergency?',\n",
    "'Provide an overview of the newborn hearing screening program?',]\n",
    "\n",
    "for question in questions:\n",
    "    print(question)\n",
    "    result = chain.invoke({\"question\" : question})\n",
    "    print(result)\n",
    "    print(result[\"response\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create synthetic testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "\n",
    "# Load the docs\n",
    "docs = []\n",
    "docs_pdf = process_directory('docs/medi-cal/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "docs.extend(docs_pdf)\n",
    "docs_excel = process_directory('docs/medi-cal/', '**/*.xlsx', UnstructuredExcelLoader, True)\n",
    "docs.extend(docs_excel)\n",
    "print(len(docs))\n",
    "    \n",
    "# # Chunk the docs \n",
    "# # chunks = chunk_docs_nltk(docs, 1500, 150)\n",
    "# chunks = chunk_docs_semantic(docs)\n",
    "\n",
    "# Set up the parameters for generating the testset\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "generator = TestsetGenerator.from_langchain(generator_llm, critic_llm, embeddings)\n",
    "distributions = {simple: 0.5, multi_context: 0.4, reasoning: 0.1}\n",
    "\n",
    "# Generate the testset and save to disk \n",
    "testset = generator.generate_with_langchain_docs(documents=docs, test_size=10, distributions=distributions)\n",
    "testset_df = testset.to_pandas()\n",
    "\n",
    "print(\"Writing synthetic_testset.csv\")\n",
    "testset_df.to_csv(\"testsets/synthetic_testset.csv\")\n",
    "testset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate synthetic dataset using Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the synthetic testset using Ragas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Run the Ragas evaluation and show the results\n",
    "# Get the questions and groundtruths from the dataframe\n",
    "testset_df = pd.read_csv(\"testsets/synthetic_testset.csv\")\n",
    "\n",
    "questions = testset_df[\"question\"].values.tolist()\n",
    "questions = [str(question) for question in questions]\n",
    "\n",
    "groundtruths = testset_df[\"ground_truth\"].values.tolist()\n",
    "groundtruths = [str(ground_truth) for ground_truth in groundtruths]  \n",
    "\n",
    "# Specify the eval metrics\n",
    "eval_metrics = [answer_correctness, answer_relevancy, context_precision, context_recall, faithfulness]\n",
    "\n",
    "# Run the Ragas evaluation and show the results\n",
    "ragas_results, ragas_results_df = run_ragas_evaluation(chain, questions, groundtruths, eval_metrics)\n",
    "\n",
    "# Write the results to disk\n",
    "print(\"Writing synthetic_testset_ragas_results.csv\")\n",
    "ragas_results_df.to_csv(\"ragas/synthetic_testset_ragas_results.csv\")\n",
    "\n",
    "# Show the resutls\n",
    "ragas_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sample ReAct agent using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", \n",
    "                             temperature=0, \n",
    "                             safety_settings={\n",
    "                                 HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                                 HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                                 HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                                 HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
    "                                 }\n",
    "                                 )\n",
    "\n",
    "tools = load_tools([\"arxiv\"])\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What's the paper 1605.08386 about?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Unstructured to parse docs and load to Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/drive/1BJYYyrPVe0_9EGyXqeNyzmVZDrCRZwsg?usp=sharing#scrollTo=CwzrH-9_K6-z\n",
    "\n",
    "%pip install -qU \"unstructured-ingest[pdf]\" unstructured faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured_ingest.v2.pipeline.pipeline import Pipeline\n",
    "from unstructured_ingest.v2.interfaces import ProcessorConfig\n",
    "from unstructured_ingest.v2.processes.connectors.local import (\n",
    "    LocalIndexerConfig,\n",
    "    LocalDownloaderConfig,\n",
    "    LocalConnectionConfig,\n",
    "    LocalUploaderConfig\n",
    ")\n",
    "from unstructured_ingest.v2.processes.partitioner import PartitionerConfig\n",
    "from unstructured_ingest.v2.processes.chunker import ChunkerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "directory_with_pdfs=\"app_2/content/data\"\n",
    "directory_with_results=\"app_2/content/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline.from_configs(\n",
    "    context=ProcessorConfig(),\n",
    "    indexer_config=LocalIndexerConfig(input_path=directory_with_pdfs),\n",
    "    downloader_config=LocalDownloaderConfig(),\n",
    "    source_connection_config=LocalConnectionConfig(),\n",
    "    partitioner_config=PartitionerConfig(\n",
    "        partition_by_api=True,\n",
    "        api_key=os.getenv(\"UNSTRUCTURED_API_KEY\"),\n",
    "        partition_endpoint=os.getenv(\"UNSTRUCTURED_API_URL\"),\n",
    "        strategy=\"hi_res\",\n",
    "        additional_partition_args={\n",
    "            \"split_pdf_page\": True,\n",
    "            \"split_pdf_concurrency_level\": 15,\n",
    "            },\n",
    "        ),\n",
    "    uploader_config=LocalUploaderConfig(output_dir=directory_with_results)\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.staging.base import elements_from_json\n",
    "\n",
    "elements = []\n",
    "for filename in os.listdir(directory_with_results):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(directory_with_results, filename)\n",
    "        try:\n",
    "            elements.extend(elements_from_json(filename=file_path))\n",
    "        except IOError:\n",
    "            print(f\"Error: Could not read file {filename}.\")\n",
    "print(len(elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "docs = []\n",
    "\n",
    "for element in elements:\n",
    "    metadata = element.metadata.to_dict()\n",
    "    docs.append(Document(page_content=element.text, metadata=metadata))\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(documents=docs, embedding=create_embeddings_vertexai())\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.save_local(\"app_2/faiss_index_unstructured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vector_store = FAISS.load_local(\"app_2/faiss_index_unstructured\", create_embeddings_vertexai(), allow_dangerous_deserialization=True)\n",
    "new_retriever = new_vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chain = create_chain('gemini-1.5-flash', chat_prompt_template, new_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['What is my benefit for acupuncture?',\n",
    "'Who should I call if I have an emergency?',\n",
    "'What is my vision benefit?',]\n",
    "\n",
    "for question in questions:\n",
    "    print(question)\n",
    "    result = chain.invoke({\"question\" : question})\n",
    "    print(result)\n",
    "    print(result[\"response\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyMuPDFLoader to parse docs and load to Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "docs_pdf = process_directory('docs/medi-cal/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "docs_excel = process_directory('docs/medi-cal/', '**/*.xlsx', UnstructuredExcelLoader, True)\n",
    "\n",
    "docs.extend(docs_pdf)\n",
    "docs.extend(docs_excel)\n",
    "\n",
    "print(len(docs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(documents=docs, embedding=create_embeddings_vertexai())\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.save_local(\"app_2/faiss_index_pymupdfloader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vector_store = FAISS.load_local(\"app_2/faiss_index_pymupdfloader\", create_embeddings_vertexai(), allow_dangerous_deserialization=True)\n",
    "new_retriever = new_vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=new_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "You are a helpful conversational agent for the State of California.\n",
    "Your expertise is fully understanding the Medi-Cal provider manuals. \n",
    "You need to answer questions posed by a member, who is trying to get answers about services provided by Medi-Cal.  \n",
    "Your goal is to provide a helpful and detailed response, in at least 2-3 sentences. \n",
    "\n",
    "You will be analyzing the health plan documents to derive a good answer, based on the following information:\n",
    "1. The question asked.\n",
    "2. The provided context, which comes from health plan document. You will need to answer the question based on the provided context and the conversation history.\n",
    "\n",
    "Now it's your turn!\n",
    "\n",
    "{question}\n",
    "\n",
    "{context}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model='gemini-1.5-flash',\n",
    "    temperature=0,\n",
    "    safety_settings={\n",
    "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        }\n",
    "    )\n",
    "        \n",
    "# new_chain = (\n",
    "#         {\"context\": itemgetter(\"question\") | new_retriever, \"question\": itemgetter(\"question\")} \n",
    "#         | RunnablePassthrough.assign(context=itemgetter(\"context\")) \n",
    "#         | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "#         )\n",
    "\n",
    "new_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")} \n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\")) \n",
    "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['What is my benefit for acupuncture?',\n",
    "'Who should I call if I have an emergency?',\n",
    "'What is my vision benefit?',]\n",
    "\n",
    "for question in questions:\n",
    "    print(question)\n",
    "    result = new_chain.invoke({\"question\" : question})\n",
    "    print(result)\n",
    "    print(result[\"response\"].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aie4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
