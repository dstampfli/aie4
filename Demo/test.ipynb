{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "\n",
    "%pip install -qU langchain-cohere langchain-community langchain-core==0.2.40 langchain_experimental langchain-google-community langchain-google-genai langchain-huggingface==0.0.3 langchain-openai langchain-qdrant langchain-google-vertexai\n",
    "%pip install -qU -q chainlit==1.1.302\n",
    "%pip install -qU -q docx2txt\n",
    "%pip install -qU google-cloud-aiplatform\n",
    "%pip install -qU google-cloud-discoveryengine\n",
    "%pip install -qU IProgress\n",
    "%pip install -qU nltk\n",
    "%pip install -qU openpyxl\n",
    "%pip install -qU pymupdf\n",
    "%pip install -qU python-dotenv\n",
    "%pip install -qU ragas==0.1.20\n",
    "%pip install -qU tqdm\n",
    "%pip install -qU unstructured \n",
    "\n",
    "# Verify installed packages have compatible dependencies\n",
    "%pip check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download punkt_tab module that is used for sentence tokenizaiton\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get environment variables\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Google user permissions\n",
    "\n",
    "import google.auth\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If running in Colab, use the permissions of the currently authenticated user\n",
    "if 'google.colab' in sys.modules:\n",
    "\tprint('Running in Google Colab')\n",
    "\tfrom google.colab import auth\n",
    "\n",
    "\tauth.authenticate_user()\n",
    "\n",
    "# If not, set the GOOGLE_APPLICATION_CREDENTIALS to the service account credentials file \n",
    "else:\n",
    "\tprint('Running locally')\n",
    "\t# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'credentials.json'\n",
    "\t# print(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
    "\n",
    "#####\n",
    "\n",
    "def test_google_perms():\n",
    "\tfrom google.cloud import storage\n",
    "\t\n",
    "\tcreds, _ = google.auth.default(quota_project_id=os.environ[\"PROJECT_ID\"])\n",
    "\n",
    "\t# Now, you can use the Google Cloud client libraries\n",
    "\tclient = storage.Client(credentials=creds)\n",
    "\n",
    "\t# List all buckets in your project\n",
    "\tbuckets = list(client.list_buckets())\n",
    "\tprint(buckets)\n",
    "\n",
    "test_google_perms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=os.environ['PROJECT_ID'], location=os.environ['REGION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verfify that our Google API key works\n",
    "\n",
    "import requests\n",
    "\n",
    "GOOGLE_API_KEY = os.environ['GOOGLE_API_KEY']\n",
    "url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={GOOGLE_API_KEY}'\n",
    "  \n",
    "headers = {\n",
    "\t\"Content-Type\": \"application/json\",\n",
    "}\n",
    "  \n",
    "data = {\"contents\":[{\"parts\":[{\"text\":\"What is MassHealth?\"}]}]}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the docs from a directory\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/file_directory/\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader \n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "\n",
    "def process_directory(path: str, glob: str, loader_cls: str, use_multithreading=True):\n",
    "\t\n",
    "\tloader = DirectoryLoader(path=path, glob=glob, show_progress=True, loader_cls=loader_cls, use_multithreading=use_multithreading)\n",
    "\t\n",
    "\tdocs = loader.load()\n",
    "\t\n",
    "\treturn docs\n",
    "\n",
    "#####\n",
    "\n",
    "def test_process_directory():\n",
    "\tdocs = []\n",
    "\t\n",
    "\tdocs_pdf = process_directory('docs/masshealth/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "\t# print(len(docs_pdf))\n",
    "\tdocs_excel = process_directory('docs/masshealth/', '**/*.xlsx', UnstructuredExcelLoader, True)\n",
    "\t# print(len(docs_excel))\n",
    "\n",
    "\tdocs.extend(docs_pdf)\n",
    "\tdocs.extend(docs_excel)\n",
    "\tprint(len(docs))\n",
    "\n",
    "test_process_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the docs from a single file\n",
    "\n",
    "# TODO - add TextLoader refernece\n",
    "# TODO - add CSVLoader reference\n",
    "# https://python.langchain.com/docs/integrations/document_loaders/pymupdf/\n",
    "# https://python.langchain.com/docs/integrations/document_loaders/microsoft_word/\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "def process_file(path: str):\n",
    "\n",
    "\tdocs = None\n",
    "\n",
    "\t# Select the right loader\n",
    "\tif 'txt' in path.lower():\n",
    "\t\tloader = TextLoader(path)\n",
    "\telif 'pdf' in path.lower():\n",
    "\t\tloader = PyMuPDFLoader(path)\n",
    "\telif 'docx' in path.lower():\n",
    "\t\tloader = Docx2txtLoader(path)\n",
    "\telse:\n",
    "\t\tprint(f'No document loader found for {path}')\n",
    "\n",
    "\tdocs = loader.load()\n",
    "\n",
    "\treturn docs\n",
    "\n",
    "#####\n",
    "\n",
    "def test_process_file():\n",
    "\tdocs = process_file(\"docs/masshealth/ACA-1-0324.pdf\")\n",
    "\tprint(docs[0].metadata)\n",
    "\n",
    "test_process_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using Hugging Face \n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def create_embeddings_huggingface(model='Snowflake/snowflake-arctic-embed-m') -> HuggingFaceEmbeddings:\n",
    "\n",
    "\t# Initialize the OpenAIEmbeddings class\n",
    "\tembeddings = HuggingFaceEmbeddings(model_name=model)\n",
    "\n",
    "\treturn embeddings\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_embeddings_huggingface():\n",
    "\ttext = 'What is my benefit for acupuncture?'\n",
    "\tembeddings = create_embeddings_huggingface()\n",
    "\tvector = embeddings.embed_query(text)\n",
    "\tprint(vector)\n",
    "\treturn embeddings\n",
    "\n",
    "test_create_embeddings_huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using OpenAI\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def create_embeddings_openai(model=\"text-embedding-ada-002\") -> OpenAIEmbeddings:\n",
    "\n",
    "\t# Initialize the OpenAIEmbeddings class\n",
    "\tembeddings = OpenAIEmbeddings(model=model)\n",
    "\n",
    "\treturn embeddings\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_embeddings_openai():\n",
    "\ttext = \"What is my benefit for acupuncture?\"\n",
    "\tembeddings = create_embeddings_openai()\n",
    "\tvector = embeddings.embed_query(text)\n",
    "\tprint(vector)\n",
    "\treturn embeddings\n",
    "\n",
    "test_create_embeddings_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using Vertex AI\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/text_embedding/google_vertex_ai_palm/\n",
    "\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "def create_embeddings_vertexai(model=\"text-embedding-004\") -> VertexAIEmbeddings:\n",
    "\n",
    "\tcreds, _ = google.auth.default(quota_project_id=os.environ[\"PROJECT_ID\"])\n",
    "\n",
    "\t# Initialize the VertexAIEmbeddings class\n",
    "\tembeddings = VertexAIEmbeddings(model_name=model, \n",
    "\t\t\t\t\t\t\t\t credentials=creds)\n",
    "\n",
    "\treturn embeddings\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_embeddings_vertexai():\n",
    "\ttext = 'What is my benefit for acupuncture?'\n",
    "\tembeddings = create_embeddings_vertexai()\n",
    "\tvector = embeddings.embed_query(text)\n",
    "\tprint(vector)\n",
    "\treturn embeddings\n",
    "\n",
    "test_create_embeddings_vertexai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty chunks \n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "def remove_empty_chunks(chunks_start: list) -> list:\n",
    "\t\n",
    "\tstart = len(chunks_start)\n",
    "\t# print(f'start - {start} chunks')\n",
    "\t\n",
    "\t# Remove empty chunks\n",
    "\tchunks_end = [chunk for chunk in chunks_start if chunk.page_content.strip()]\n",
    "\n",
    "\tend = len(chunks_end)\n",
    "\t# print(f'end - {end} chunks')\n",
    "\n",
    "\treturn chunks_end   \n",
    "\n",
    "#####\n",
    "\n",
    "def test_remove_empty_chunks():\n",
    "\tdocs = process_file(\"docs/masshealth/ACA-1-0324.pdf\")\n",
    "\n",
    "\t# Added a test doc\n",
    "\tdoc = Document(\n",
    "\t\tpage_content='',\n",
    "\t\tmetadata=docs[0].metadata\n",
    "\t)\n",
    "\tdocs.append(doc)\n",
    "\tprint(len(docs))\n",
    "\n",
    "\t# Remove the empty doc (chunk)\n",
    "\tdocs = remove_empty_chunks(docs)\n",
    "\tprint(len(docs))\n",
    "\n",
    "test_remove_empty_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter using recursive character text splitter\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_docs_recursive(documents: list, chunk_size=500, chunk_overlap=50) -> list:\n",
    "\n",
    "\ttext_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "\tchunks_start = text_splitter.split_documents(documents)\n",
    "\n",
    "\tchunks_end = remove_empty_chunks(chunks_start=chunks_start)\n",
    "\n",
    "\treturn chunks_end\n",
    "\n",
    "#####\n",
    "\n",
    "def test_chunk_docs_recursive(): \n",
    "\tdocs = process_file(\"docs/masshealth/ACA-1-0324.pdf\")\n",
    "\tchunks = chunk_docs_recursive(documents=docs)\n",
    "\tprint(len(chunks))\n",
    "\tprint(chunks[0].page_content)\n",
    "\n",
    "test_chunk_docs_recursive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter using NLTK\n",
    "\n",
    "# https://python.langchain.com/docs/how_to/split_by_token/\n",
    "\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "def chunk_docs_nltk(documents: list, chunk_size=512, chunk_overlap=64) -> list:\n",
    "\n",
    "\ttext_splitter = NLTKTextSplitter(\n",
    "\tchunk_size=chunk_size, \n",
    "\tchunk_overlap=chunk_overlap)\n",
    "\n",
    "\tchunks_start = text_splitter.split_documents(documents)\n",
    "\n",
    "\tchunks_end = remove_empty_chunks(chunks_start=chunks_start)\n",
    "\n",
    "\treturn chunks_end\n",
    "\n",
    "#####\n",
    "\n",
    "def test_chunk_docs_nltk(): \n",
    "\tdocs = process_file(\"docs/masshealth/ACA-1-0324.pdf\")\n",
    "\tchunks = chunk_docs_nltk(documents=docs)\n",
    "\tprint(len(chunks))\n",
    "\tprint(chunks[0].page_content)\n",
    "\n",
    "test_chunk_docs_nltk()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter semantic chunking \n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/semantic-chunker/\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "def chunk_docs_semantic(documents: list, ) -> list:\n",
    "\n",
    "\t# TODO - Use embeddings parameter\n",
    "\ttext_splitter = SemanticChunker(create_embeddings_openai(), breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "\tchunks_start = text_splitter.split_documents(documents)\n",
    "\n",
    "\t# Remove empty chunks\n",
    "\tchunks_end = remove_empty_chunks(chunks_start)\n",
    "\n",
    "\treturn chunks_end\n",
    "\n",
    "#####\n",
    "\n",
    "def test_chunk_docs_semantic():\n",
    "\tdocs = process_file(\"docs/masshealth/ACA-1-0324.pdf\")\n",
    "\tchunks = chunk_docs_semantic(docs)\n",
    "\tprint(len(chunks))\n",
    "\tprint(chunks[0].page_content)\n",
    "\n",
    "test_chunk_docs_semantic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant vector store\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "def create_qdrant_vector_store(location: str, collection_name: str, vector_size: int, embeddings: Embeddings, documents: list) -> QdrantVectorStore:\n",
    "\n",
    "\t# Initialize the Qdrant client\n",
    "\tqdrant_client = QdrantClient(location=location)\n",
    "\n",
    "\t# Create a collection in Qdrant\n",
    "\tqdrant_client.create_collection(collection_name=collection_name, vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE))\n",
    "\n",
    "\t# Initialize QdrantVectorStore with the Qdrant client\n",
    "\tqdrant_vector_store = QdrantVectorStore(client=qdrant_client, collection_name=collection_name, embedding=embeddings)\n",
    "\t\n",
    "\tqdrant_vector_store.add_documents(documents)\n",
    "\t\n",
    "\treturn qdrant_vector_store\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_qdrant_vector_store():\n",
    "\tembeddings = create_embeddings_openai()\n",
    "\tdocs = process_file(\"docs/masshealth/ACA-1-0324.pdf\")\n",
    "\tprint(len(docs))\n",
    "\tchunks = chunk_docs_recursive(docs)\n",
    "\tprint(len(chunks))\n",
    "\tvector_store = create_qdrant_vector_store(\":memory:\", \"masshealth-test\", 1536, embeddings, chunks)\n",
    "\tprint(vector_store.collection_name)\n",
    "\n",
    "test_create_qdrant_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant retriever\n",
    "\n",
    "# TODO - Add reference \n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "def create_retriever_qdrant(vector_store: QdrantVectorStore) -> BaseRetriever:\n",
    "\n",
    "\tretriever = vector_store.as_retriever()\n",
    "\n",
    "\treturn retriever\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_retriever_qdrant(text: str = None):\n",
    "\tembeddings = create_embeddings_openai()\n",
    "\tdocs = process_file(\"docs/masshealth/ACA-1-0324.pdf\")\n",
    "\tchunks = chunk_docs_recursive(docs)\n",
    "\tvector_store = create_qdrant_vector_store(\":memory:\", \"masshealth-test\", 1536, embeddings, chunks)\n",
    "\tretriever = create_retriever_qdrant(vector_store)\n",
    "\tif text:\n",
    "\t\tdocs = retriever.invoke(text)\n",
    "\t\tprint(docs[0])\n",
    "\n",
    "print('\\nQDRANT')\n",
    "test_create_retriever_qdrant('What is my benefit for acupuncture?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Vertex AI retriever\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/retrievers/google_vertex_ai_search/\n",
    "\n",
    "from langchain_google_community import VertexAISearchRetriever\n",
    "\n",
    "def create_retriever_vertexai() -> VertexAISearchRetriever:\n",
    "\n",
    "\tretriever = VertexAISearchRetriever(project_id=os.environ['PROJECT_ID'], \n",
    "\t\t\t\t\t\t\t\t\t location_id=os.environ['LOCATION_ID'], \n",
    "\t\t\t\t\t\t\t\t\t data_store_id=os.environ['DATA_STORE_ID'], \n",
    "\t\t\t\t\t\t\t\t\t max_documents=3)\n",
    "\n",
    "\treturn retriever\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_retriever_vertexai(text: str = None):\n",
    "\tretriever = create_retriever_vertexai()\n",
    "\tif text:\n",
    "\t\tdocs = retriever.invoke(text)\n",
    "\t\tprint(docs[0])\n",
    "\n",
    "print('\\nVERTEX AI')\n",
    "test_create_retriever_vertexai('What is my benefit for acupuncture?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/#chatprompttemplate\n",
    "# https://python.langchain.com/v0.2/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def create_chat_prompt_template(template: str = None) -> ChatPromptTemplate:\n",
    "\t\n",
    "\tif template is None:\n",
    "\t\ttemplate = '''\n",
    "\t\tYou are a helpful conversational agent for the State of California.\n",
    "\t\tYour expertise is fully understanding the Medi-Cal provider manuals. \n",
    "\t\tYou need to answer questions posed by a member, who is trying to get answers about services provided by Medi-Cal.  \n",
    "\t\tYour goal is to provide a helpful and detailed response, in at least 2-3 sentences. \n",
    "\n",
    "\t\tYou will be analyzing the health plan documents to derive a good answer, based on the following information:\n",
    "\t\t1. The question asked.\n",
    "\t\t2. The provided context, which comes from health plan document. You will need to answer the question based on the provided context and the conversation history.\n",
    "\n",
    "\t\tNow it's your turn!\n",
    "\n",
    "\t\t{question}\n",
    "\n",
    "\t\t{context}\n",
    "\t\t'''\n",
    "\t\n",
    "\tprompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\treturn prompt\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_chat_prompt_template():\n",
    "\tprompt = create_chat_prompt_template()\n",
    "\tprint(prompt)\n",
    "\n",
    "test_create_chat_prompt_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Langchain chain..\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/llms/google_ai/\n",
    "# https://python.langchain.com/docs/integrations/chat/google_generative_ai/\n",
    "# https://ai.google.dev/gemini-api/docs/safety-settings \n",
    "\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableSerializable\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from operator import itemgetter\n",
    "\n",
    "def create_chain (model_name: str, \n",
    "\t\t\t\t  prompt_template: ChatPromptTemplate, \n",
    "\t\t\t\t  retriever: BaseRetriever) -> RunnableSerializable:\n",
    "\n",
    "\tif \"gemini\" in model_name.lower():\n",
    "\t\tllm = ChatGoogleGenerativeAI(\n",
    "\t\t\tmodel=model_name,\n",
    "\t\t\ttemperature=0,\n",
    "\t\t\tsafety_settings={\n",
    "\t\t\t\t\tHarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "\t\t\t\t\tHarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "\t\t\t\t\tHarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "\t\t\t\t\tHarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "\t\t\t\t},\n",
    "\t\t\t)\n",
    "\telse:\n",
    "\t\tprint(\"Unsuported model name\")\n",
    "\t\t\n",
    "\tchain = (\n",
    "\t\t{\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")} \n",
    "\t\t| RunnablePassthrough.assign(context=itemgetter(\"context\")) \n",
    "\t\t| {\"response\": prompt_template | llm, \"context\": itemgetter(\"context\")}\n",
    "\t\t)\n",
    "\n",
    "\treturn chain\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_chain_qdrant():\n",
    "\tembeddings = create_embeddings_openai()\n",
    "\tdocs = process_file('docs/medi-cal/acu.pdf')\n",
    "\tchunks = chunk_docs_recursive(docs)\n",
    "\tvector_store = create_qdrant_vector_store(\":memory:\", \"test\", 1536, embeddings, chunks)\n",
    "\tretriever = create_retriever_qdrant(vector_store)\n",
    "\tchat_prompt_template = create_chat_prompt_template()\n",
    "\tchain = create_chain('gemini-1.5-flash', chat_prompt_template, retriever)\n",
    "\tresult = chain.invoke({'question' : 'What is my benefit for acupuncture?'})\n",
    "\tprint(result)\n",
    "\n",
    "print('\\nQDRANT')\n",
    "test_create_chain_qdrant()\n",
    "\n",
    "###\n",
    "\n",
    "def test_create_chain_vertexai():\n",
    "\tretreiver = create_retriever_vertexai()\n",
    "\tchat_prompt_template = create_chat_prompt_template()\n",
    "\tchain = create_chain('gemini-1.5-flash', chat_prompt_template, retreiver)\n",
    "\tresult = chain.invoke({'question' : 'What is my benefit for acupuncture?'})\n",
    "\tprint(result)\n",
    "\n",
    "print('\\nVERTEX AI')\n",
    "test_create_chain_vertexai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers from a chain usin a list of questions\n",
    "\n",
    "import json\n",
    "from langchain_core.runnables import RunnableSerializable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "def generate_answers_contexts(chain: RunnableSerializable, \n",
    "\t\t\t\t\t\t\t  questions: list):\n",
    "\t\n",
    "\tanswers = []\n",
    "\tcontexts = []\n",
    "\n",
    "\t# Loop over the list of questions and call the chain to get the answer and context\n",
    "\tfor question in questions:\n",
    "\t\tprint(question)\n",
    "\n",
    "\t\t# Call the chain to get answers and contexts\n",
    "\t\tresponse = chain.invoke({\"question\" : question})\n",
    "\t\tprint(response)\n",
    "\n",
    "\t\t# Capture the answer and context \n",
    "\t\tanswers.append(response[\"response\"].content)\n",
    "\t\tcontexts.append([context.page_content for context in response[\"context\"]])\n",
    "\n",
    "\treturn answers, contexts\n",
    "\n",
    "#####\n",
    "\n",
    "def test_generate_answers_contexts():\n",
    "\tretreiver = create_retriever_vertexai()\n",
    "\tchat_prompt_template = create_chat_prompt_template()\n",
    "\tchain = create_chain(model_name=\"gemini-1.5-flash\", \n",
    "\t\t\t\t\t  prompt_template=chat_prompt_template,\n",
    "\t\t\t\t\t  retriever=retreiver)\n",
    "\t\n",
    "\tquestions = [\"What is my benefit for acupuncture?\",\n",
    "\t\t\t  \"Who should I call if I have an emergency?\",\n",
    "\t\t\t  \"Provide an overview of the newborn hearing screening program?\"]\n",
    "\t\n",
    "\tanswers, contexts = generate_answers_contexts(chain=chain, questions=questions)\n",
    "\t\n",
    "\tprint(f\"Total number of answers = {len(answers)}\")\n",
    "\tprint(f\"Total number of contexts = {len(contexts)}\")\n",
    "\n",
    "test_generate_answers_contexts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Ragas evaluation \n",
    "\n",
    "from datasets import Dataset\n",
    "from langchain_core.runnables import RunnableSerializable\n",
    "from pandas import DataFrame\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (faithfulness, answer_relevancy, answer_correctness, context_recall, context_precision)\n",
    "\n",
    "def run_ragas_evaluation(chain: RunnableSerializable, \n",
    "\t\t\t\t\t\t questions: list, \n",
    "\t\t\t\t\t\t groundtruths: list, \n",
    "\t\t\t\t\t\t eval_metrics: list = [answer_correctness, answer_relevancy, context_recall, context_precision, faithfulness]):\n",
    "\t\n",
    "\tanswers = []\n",
    "\tcontexts = []\n",
    "\tanswers, contexts = generate_answers_contexts(chain=chain, \n",
    "\t\t\t\t\t\t\t\t\t\t\t   questions=questions)\n",
    "\n",
    "\t# Create the input dataset \n",
    "\tinput_dataset = Dataset.from_dict({\n",
    "\t\"question\" : questions,         # From the dataframe\n",
    "\t\"answer\" : answers,             # From the chain\n",
    "\t\"contexts\" : contexts,          # From the chain\n",
    "\t\"ground_truth\" : groundtruths   # From the dataframe\n",
    "\t})\n",
    "\n",
    "\t# Run the Ragas evaluation using the input dataset and eval metrics\n",
    "\tragas_results = evaluate(input_dataset, eval_metrics)\n",
    "\tragas_results_df = ragas_results.to_pandas()\n",
    "\t\n",
    "\treturn ragas_results, ragas_results_df\n",
    "\t\n",
    "#####\n",
    "\n",
    "def test_run_ragas_evaluation():\n",
    "\tprint(\"test\")    \n",
    "\n",
    "test_run_ragas_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Vertex AI Search datastore using HTTP Post\n",
    "\n",
    "import json\n",
    "import google.auth\n",
    "from google.auth.transport.requests import Request\n",
    "import requests\n",
    "\n",
    "def query_chunks(query: str, \n",
    "\t\t\t\t n: int=5):\n",
    "\t\t\n",
    "\tPROJECT_ID = os.environ['PROJECT_ID']\n",
    "\tLOCATION_ID = os.environ['LOCATION_ID']\n",
    "\tDATA_STORE_ID = os.environ['DATA_STORE_ID']\n",
    "\n",
    "\tif LOCATION_ID == 'us':\n",
    "\t\tapi_endpoint = 'us-discoveryengine.googleapis.com'\n",
    "\telse:\n",
    "\t\tapi_endpoint = 'discoveryengine.googleapis.com'\n",
    "\n",
    "\turl = f\"https://{api_endpoint}/v1alpha/projects/{PROJECT_ID}/locations/{LOCATION_ID}/collections/default_collection/dataStores/{DATA_STORE_ID}/servingConfigs/default_search:search\"\n",
    "\tprint(url)\n",
    "\t\n",
    "\theaders = {\n",
    "\t\t\t\"Authorization\": f\"Bearer {access_token}\",\n",
    "\t\t\t\"Content-Type\": \"application/json\",\n",
    "\t}\n",
    "\t\n",
    "\tpost_data = {\n",
    "\t\t\t\"servingConfig\": f\"projects/{PROJECT_ID}/locations/{LOCATION_ID}/collections/default_collection/dataStores/{DATA_STORE_ID}/servingConfigs/default_search\",\n",
    "\t\t\t\"pageSize\": n,\n",
    "\t\t\t\"query\": query,\n",
    "\t\t\t\"contentSearchSpec\": {\"searchResultMode\": \"CHUNKS\"},\n",
    "\t}\n",
    "\t\n",
    "\tresponse = requests.post(url, headers=headers, json=post_data)\n",
    "\n",
    "\tif response.status_code != 200:\n",
    "\t\tprint(\n",
    "\t\t\t\tf\"Error retrieving search results: {response.status_code} -\"\n",
    "\t\t\t\tf\" {response.text}\"\n",
    "\t\t)\n",
    "\n",
    "\treturn response.json()\n",
    "\n",
    "#####\n",
    "\n",
    "def test_query_chunks():\n",
    "\tcreds, _ = google.auth.default()\n",
    "\tcreds.refresh(Request())\n",
    "\taccess_token = creds.token\n",
    "\t# print(access_token)\n",
    "\tresponse = query_chunks('What is my benefit for acupuncture?')\n",
    "\tprint(json.dumps(response, indent=4))\n",
    "\n",
    "test_query_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RAG chain using Google Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RAG chain using Vertex AI Agent Builder datastore\n",
    "\n",
    "retreiver = create_retriever_vertexai()\n",
    "chat_prompt_template = create_chat_prompt_template()\n",
    "chain = create_chain(model_name='gemini-1.5-flash', \n",
    "                     prompt_template=chat_prompt_template, \n",
    "                     retriever=retreiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the chain \n",
    "\n",
    "questions = ['What is my benefit for acupuncture?',\n",
    "'Who should I call if I have an emergency?',\n",
    "'Provide an overview of the newborn hearing screening program?',]\n",
    "\n",
    "for question in questions:\n",
    "\tprint(question)\n",
    "\tresult = chain.invoke({\"question\" : question})\n",
    "\tprint(result)\n",
    "\tprint(result[\"response\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create synthetic testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.ragas.io/en/latest/references/generate/\n",
    "\n",
    "import google.auth\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_google_vertexai import ChatVertexAI, VertexAIEmbeddings\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from ragas.testset.generator import RunConfig\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "\n",
    "testset_name = \"masshealth_testset.csv\"\n",
    "use_chunking = False\n",
    "use_vertexai = True\n",
    "\n",
    "# Load the docs\n",
    "docs = []\n",
    "docs_pdf = process_directory('docs/masshealth/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "docs.extend(docs_pdf)\n",
    "print(len(docs))\n",
    "\n",
    "if use_chunking:\n",
    "\t# Chunk the docs \n",
    "\tdocs = chunk_docs_recursive(docs)\n",
    "\t# docs = chunk_docs_nltk(docs)\n",
    "\t# docs = chunk_docs_semantic(docs)\n",
    "\n",
    "# Set up the parameters for generating the testset\n",
    "if use_vertexai:\n",
    "\ttestset_name = \"masshealth_vertexai_testset_2.csv\"\n",
    "\n",
    "\t# https://docs.ragas.io/en/v0.1.21/howtos/customisations/gcp-vertexai.html\n",
    "\tcreds, _ = google.auth.default(quota_project_id=os.environ[\"PROJECT_ID\"])\n",
    "\tgenerator_llm = ChatVertexAI(credentials=creds,\n",
    "\t\t\t\t\t\t\t  model_name=\"gemini-1.5-pro\")\n",
    "\tcritic_llm = ChatVertexAI(credentials=creds,\n",
    "\t\t\t\t\t\t\tmodel_name=\"gemini-1.0-pro\")\n",
    "\tembeddings = VertexAIEmbeddings(credentials=creds, \n",
    "\t\t\t\t\t\t\t\t model_name=\"textembedding-gecko\")\n",
    "else:\n",
    "\ttestset_name = \"masshealth_openai_testset.csv\"\n",
    "\n",
    "\tgenerator_llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\tcritic_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\tembeddings = OpenAIEmbeddings()\n",
    "\n",
    "# https://docs.ragas.io/en/v0.1.21/howtos/customisations/run_config.html\n",
    "run_config=RunConfig(\n",
    "\tmax_workers=2\t# default: 16 but smaller number is required to avoid rate limits\n",
    "\t)\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(generator_llm=generator_llm, \n",
    "\t\t\t\t\t\t\t\t\t\t\tcritic_llm=critic_llm, \n",
    "\t\t\t\t\t\t\t\t\t\t\tembeddings=embeddings)\n",
    "\n",
    "test_size=10\n",
    "\n",
    "distributions = {simple: 0.5, \n",
    "\t\t\t\t multi_context: 0.4, \n",
    "\t\t\t\t reasoning: 0.1}\n",
    "\n",
    "# Generate the testset and save to disk \n",
    "testset = generator.generate_with_langchain_docs(documents=docs, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t test_size=test_size, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t distributions=distributions,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t run_config=run_config)\n",
    "\n",
    "# Write the testet to disk \n",
    "print(f\"Writing {testset_name}\")\n",
    "testset_df = testset.to_pandas()\n",
    "testset_df.to_csv(f\"testsets/{testset_name}\")\n",
    "testset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate synthetic dataset using Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the synthetic testset using Ragas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Run the Ragas evaluation and show the results\n",
    "# Get the questions and groundtruths from the dataframe\n",
    "testset_df = pd.read_csv(\"testsets/synthetic_testset.csv\")\n",
    "\n",
    "questions = testset_df[\"question\"].values.tolist()\n",
    "questions = [str(question) for question in questions]\n",
    "\n",
    "groundtruths = testset_df[\"ground_truth\"].values.tolist()\n",
    "groundtruths = [str(ground_truth) for ground_truth in groundtruths]  \n",
    "\n",
    "# Specify the eval metrics\n",
    "eval_metrics = [answer_correctness, answer_relevancy, context_precision, context_recall, faithfulness]\n",
    "\n",
    "# Run the Ragas evaluation and show the results\n",
    "ragas_results, ragas_results_df = run_ragas_evaluation(chain, questions, groundtruths, eval_metrics)\n",
    "\n",
    "# Write the results to disk\n",
    "print(\"Writing synthetic_testset_ragas_results.csv\")\n",
    "ragas_results_df.to_csv(\"ragas/synthetic_testset_ragas_results.csv\")\n",
    "\n",
    "# Show the resutls\n",
    "ragas_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sample ReAct agent using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", \n",
    "\t\t\t\t\t\t\t temperature=0, \n",
    "\t\t\t\t\t\t\t safety_settings={\n",
    "\t\t\t\t\t\t\t\t HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "\t\t\t\t\t\t\t\t HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "\t\t\t\t\t\t\t\t HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "\t\t\t\t\t\t\t\t HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
    "\t\t\t\t\t\t\t\t }\n",
    "\t\t\t\t\t\t\t\t )\n",
    "\n",
    "tools = load_tools([\"arxiv\"])\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What's the paper 1605.08386 about?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Unstructured to parse docs and load to Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/drive/1BJYYyrPVe0_9EGyXqeNyzmVZDrCRZwsg?usp=sharing#scrollTo=CwzrH-9_K6-z\n",
    "\n",
    "%pip install -qU \"unstructured-ingest[pdf]\" unstructured faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured_ingest.v2.pipeline.pipeline import Pipeline\n",
    "from unstructured_ingest.v2.interfaces import ProcessorConfig\n",
    "from unstructured_ingest.v2.processes.connectors.local import (\n",
    "\tLocalIndexerConfig,\n",
    "\tLocalDownloaderConfig,\n",
    "\tLocalConnectionConfig,\n",
    "\tLocalUploaderConfig\n",
    ")\n",
    "from unstructured_ingest.v2.processes.partitioner import PartitionerConfig\n",
    "from unstructured_ingest.v2.processes.chunker import ChunkerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "directory_with_pdfs=\"app_2/content/data\"\n",
    "directory_with_results=\"app_2/content/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline.from_configs(\n",
    "\tcontext=ProcessorConfig(),\n",
    "\tindexer_config=LocalIndexerConfig(input_path=directory_with_pdfs),\n",
    "\tdownloader_config=LocalDownloaderConfig(),\n",
    "\tsource_connection_config=LocalConnectionConfig(),\n",
    "\tpartitioner_config=PartitionerConfig(\n",
    "\t\tpartition_by_api=True,\n",
    "\t\tapi_key=os.getenv(\"UNSTRUCTURED_API_KEY\"),\n",
    "\t\tpartition_endpoint=os.getenv(\"UNSTRUCTURED_API_URL\"),\n",
    "\t\tstrategy=\"hi_res\",\n",
    "\t\tadditional_partition_args={\n",
    "\t\t\t\"split_pdf_page\": True,\n",
    "\t\t\t\"split_pdf_concurrency_level\": 15,\n",
    "\t\t\t},\n",
    "\t\t),\n",
    "\tuploader_config=LocalUploaderConfig(output_dir=directory_with_results)\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.staging.base import elements_from_json\n",
    "\n",
    "elements = []\n",
    "for filename in os.listdir(directory_with_results):\n",
    "\tif filename.endswith('.json'):\n",
    "\t\tfile_path = os.path.join(directory_with_results, filename)\n",
    "\t\ttry:\n",
    "\t\t\telements.extend(elements_from_json(filename=file_path))\n",
    "\t\texcept IOError:\n",
    "\t\t\tprint(f\"Error: Could not read file {filename}.\")\n",
    "print(len(elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "docs = []\n",
    "\n",
    "for element in elements:\n",
    "\tmetadata = element.metadata.to_dict()\n",
    "\tdocs.append(Document(page_content=element.text, metadata=metadata))\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(documents=docs, embedding=create_embeddings_vertexai())\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.save_local(\"app_2/faiss_index_unstructured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vector_store = FAISS.load_local(\"app_2/faiss_index_unstructured\", create_embeddings_vertexai(), allow_dangerous_deserialization=True)\n",
    "new_retriever = new_vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chain = create_chain('gemini-1.5-flash', chat_prompt_template, new_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['What is my benefit for acupuncture?',\n",
    "'Who should I call if I have an emergency?',\n",
    "'What is my vision benefit?',]\n",
    "\n",
    "for question in questions:\n",
    "\tprint(question)\n",
    "\tresult = chain.invoke({\"question\" : question})\n",
    "\tprint(result)\n",
    "\tprint(result[\"response\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyMuPDFLoader to parse docs and load to Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "docs_pdf = process_directory('docs/medi-cal/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "docs_excel = process_directory('docs/medi-cal/', '**/*.xlsx', UnstructuredExcelLoader, True)\n",
    "\n",
    "docs.extend(docs_pdf)\n",
    "docs.extend(docs_excel)\n",
    "\n",
    "print(len(docs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(documents=docs, embedding=create_embeddings_vertexai())\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.save_local(\"app_2/faiss_index_pymupdfloader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vector_store = FAISS.load_local(\"app_2/faiss_index_pymupdfloader\", create_embeddings_vertexai(), allow_dangerous_deserialization=True)\n",
    "new_retriever = new_vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=new_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "You are a helpful conversational agent for the State of California.\n",
    "Your expertise is fully understanding the Medi-Cal provider manuals. \n",
    "You need to answer questions posed by a member, who is trying to get answers about services provided by Medi-Cal.  \n",
    "Your goal is to provide a helpful and detailed response, in at least 2-3 sentences. \n",
    "\n",
    "You will be analyzing the health plan documents to derive a good answer, based on the following information:\n",
    "1. The question asked.\n",
    "2. The provided context, which comes from health plan document. You will need to answer the question based on the provided context and the conversation history.\n",
    "\n",
    "Now it's your turn!\n",
    "\n",
    "{question}\n",
    "\n",
    "{context}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "\tmodel='gemini-1.5-flash',\n",
    "\ttemperature=0,\n",
    "\tsafety_settings={\n",
    "\t\t\tHarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "\t\t\tHarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "\t\t\tHarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "\t\t\tHarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "\t\t}\n",
    "\t)\n",
    "\t\t\n",
    "# new_chain = (\n",
    "#         {\"context\": itemgetter(\"question\") | new_retriever, \"question\": itemgetter(\"question\")} \n",
    "#         | RunnablePassthrough.assign(context=itemgetter(\"context\")) \n",
    "#         | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "#         )\n",
    "\n",
    "new_chain = (\n",
    "\t\t{\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")} \n",
    "\t\t| RunnablePassthrough.assign(context=itemgetter(\"context\")) \n",
    "\t\t| {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "\t\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['What is my benefit for acupuncture?',\n",
    "'Who should I call if I have an emergency?',\n",
    "'What is my vision benefit?',]\n",
    "\n",
    "for question in questions:\n",
    "\tprint(question)\n",
    "\tresult = new_chain.invoke({\"question\" : question})\n",
    "\tprint(result)\n",
    "\tprint(result[\"response\"].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aie4-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
